{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE Table Retrieval\n",
    "\n",
    "Hypothetical Document Embeddings for table search:\n",
    "1. Generate table descriptions using LLM\n",
    "2. Generate hypothetical table descriptions from queries using LLM\n",
    "3. Encode and retrieve using MiniLM-L6-v2 + FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: Add LLM library imports (e.g., openai, anthropic, etc.)\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables\n",
    "tables_df = pd.read_csv('data/wikitables_mini.csv')\n",
    "print(f\"Loaded {len(tables_df)} tables\")\n",
    "print(f\"Columns: {list(tables_df.columns)}\")\n",
    "tables_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Table Descriptions (HyDE)\n",
    "\n",
    "Use LLM to generate natural descriptions from table metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_description_with_llm(row, max_chars=256):\n",
    "    \"\"\"\n",
    "    Generate table description using LLM.\n",
    "    \n",
    "    TODO: Replace this with actual LLM call.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare table metadata as prompt context\n",
    "    metadata = []\n",
    "    if pd.notna(row['table_caption']):\n",
    "        metadata.append(f\"Caption: {row['table_caption']}\")\n",
    "    if pd.notna(row['page_title']):\n",
    "        metadata.append(f\"Page: {row['page_title']}\")\n",
    "    if pd.notna(row['section_title']):\n",
    "        metadata.append(f\"Section: {row['section_title']}\")\n",
    "    \n",
    "    try:\n",
    "        headers = json.loads(row['headers'])\n",
    "        if headers:\n",
    "            metadata.append(f\"Columns: {', '.join([str(h) for h in headers[:10]])}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        sample_data = json.loads(row['sample_data'])\n",
    "        if sample_data and len(sample_data) > 0:\n",
    "            sample_str = str(sample_data[0][:5])\n",
    "            metadata.append(f\"Sample: {sample_str}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    metadata_str = '\\n'.join(metadata)\n",
    "    \n",
    "    # ============================================================\n",
    "    # TODO: LLM CALL HERE\n",
    "    # ============================================================\n",
    "    # Prompt: \"Given the following table metadata, write a natural \n",
    "    # description of what this table contains in 1-2 sentences.\n",
    "    # Keep it under 256 characters.\\n\\n{metadata_str}\"\n",
    "    #\n",
    "    # Example LLM call:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes tables.\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Given the following table metadata, write a natural description of what this table contains in 1-2 sentences. Keep it under 256 characters.\\n\\n{metadata_str}\"}\n",
    "    #     ],\n",
    "    #     max_tokens=100,\n",
    "    #     temperature=0.3\n",
    "    # )\n",
    "    # description = response.choices[0].message.content.strip()\n",
    "    # ============================================================\n",
    "    \n",
    "    # Fallback: concatenate metadata (replace with actual LLM response)\n",
    "    description = ' '.join(metadata)\n",
    "    \n",
    "    return description[:max_chars]\n",
    "\n",
    "# Test\n",
    "print(\"Example LLM-generated descriptions:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(3):\n",
    "    desc = generate_table_description_with_llm(tables_df.iloc[i])\n",
    "    print(f\"\\n{i+1}. {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptions for all tables\n",
    "print(\"Generating LLM descriptions for all tables...\")\n",
    "print(\"NOTE: This will make ~3K LLM API calls. Consider batch processing or caching.\")\n",
    "print()\n",
    "\n",
    "table_descriptions = []\n",
    "table_ids = []\n",
    "\n",
    "for idx, row in tqdm(tables_df.iterrows(), total=len(tables_df)):\n",
    "    table_descriptions.append(generate_table_description_with_llm(row))\n",
    "    table_ids.append(row['table_id'])\n",
    "\n",
    "print(f\"Generated {len(table_descriptions)} descriptions\")\n",
    "print(f\"Length stats - Mean: {np.mean([len(d) for d in table_descriptions]):.1f}, Max: {max([len(d) for d in table_descriptions])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "print(f\"Loading {model_name}...\")\n",
    "encoder = SentenceTransformer(model_name)\n",
    "print(f\"Dimension: {encoder.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode descriptions\n",
    "print(\"Encoding...\")\n",
    "table_embeddings = encoder.encode(\n",
    "    table_descriptions, batch_size=32, show_progress_bar=True,\n",
    "    convert_to_numpy=True, normalize_embeddings=True\n",
    ")\n",
    "print(f\"Shape: {table_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "print(\"Building index...\")\n",
    "index = faiss.IndexFlatIP(encoder.get_sentence_embedding_dimension())\n",
    "index.add(table_embeddings.astype('float32'))\n",
    "print(f\"✓ Index built with {index.ntotal} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Custom Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom queries - change test_query and run\n",
    "test_query = \"olympic medals table\"\n",
    "top_k = 5\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================\n",
    "# TODO: LLM CALL HERE - Generate hypothetical table description from query\n",
    "# ============================================================\n",
    "# Prompt: \"Given the search query: '{test_query}', generate a description \n",
    "# of what a relevant table would contain. Describe the table structure, \n",
    "# columns, and type of data it would have. Keep it under 256 characters.\"\n",
    "#\n",
    "# Example LLM call:\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-4\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"Given the search query: '{test_query}', generate a description of what a relevant table would contain. Describe the table structure, columns, and type of data it would have. Keep it under 256 characters.\"}\n",
    "#     ],\n",
    "#     max_tokens=100,\n",
    "#     temperature=0.3\n",
    "# )\n",
    "# hypothetical_description = response.choices[0].message.content.strip()\n",
    "# ============================================================\n",
    "\n",
    "# Fallback: use query directly (replace with LLM-generated description)\n",
    "hypothetical_description = test_query\n",
    "print(f\"Hypothetical table description: {hypothetical_description}\\n\")\n",
    "\n",
    "# Encode and search\n",
    "test_emb = encoder.encode([hypothetical_description], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "scores_test, indices_test = index.search(test_emb, top_k)\n",
    "\n",
    "for rank in range(top_k):\n",
    "    idx = indices_test[0][rank]\n",
    "    table_id = table_ids[idx]\n",
    "    row = tables_df[tables_df['table_id'] == table_id].iloc[0]\n",
    "    \n",
    "    print(f\"\\n{rank + 1}. {table_id} (score: {scores_test[0][rank]:.4f})\")\n",
    "    print(f\"   Page: {row['page_title']}\")\n",
    "    print(f\"   Caption: {row['table_caption']}\")\n",
    "    print(f\"   Description: {table_descriptions[idx][:120]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Queries and Relevance Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "queries = {}\n",
    "with open('data/queries.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(None, 1)\n",
    "        if len(parts) == 2:\n",
    "            query_id, query_text = parts\n",
    "            queries[query_id] = query_text\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "print(\"Examples:\", list(queries.items())[:3])\n",
    "\n",
    "# Load qrels (relevance judgments)\n",
    "qrels = defaultdict(dict)\n",
    "with open('data/qrels.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 4:\n",
    "            query_id, table_id, relevance = parts[0], parts[2], int(parts[3])\n",
    "            qrels[query_id][table_id] = relevance\n",
    "\n",
    "qrels = dict(qrels)\n",
    "print(f\"Loaded qrels for {len(qrels)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Hypothetical Descriptions from Queries (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"Generating hypothetical table descriptions for {len(query_texts)} queries...\")\n",
    "print(\"NOTE: This will make ~60 LLM API calls.\")\n",
    "print()\n",
    "\n",
    "hypothetical_descriptions = []\n",
    "\n",
    "for query_text in tqdm(query_texts):\n",
    "    # ============================================================\n",
    "    # TODO: LLM CALL HERE - Generate hypothetical table description\n",
    "    # ============================================================\n",
    "    # Prompt: \"Given the search query: '{query_text}', generate a description \n",
    "    # of what a relevant table would contain. Describe the table structure, \n",
    "    # columns, and type of data it would have. Keep it under 256 characters.\"\n",
    "    #\n",
    "    # Example LLM call:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Given the search query: '{query_text}', generate a description of what a relevant table would contain. Describe the table structure, columns, and type of data it would have. Keep it under 256 characters.\"}\n",
    "    #     ],\n",
    "    #     max_tokens=100,\n",
    "    #     temperature=0.3\n",
    "    # )\n",
    "    # hyp_desc = response.choices[0].message.content.strip()\n",
    "    # ============================================================\n",
    "    \n",
    "    # Fallback: use query directly (replace with LLM response)\n",
    "    hyp_desc = query_text\n",
    "    hypothetical_descriptions.append(hyp_desc)\n",
    "\n",
    "print(f\"Generated {len(hypothetical_descriptions)} hypothetical descriptions\")\n",
    "print(f\"\\nExamples:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Query: {query_texts[i]}\")\n",
    "    print(f\"  Hypothetical: {hypothetical_descriptions[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Hypothetical Descriptions and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode hypothetical descriptions (NOT raw queries)\n",
    "print(f\"Encoding {len(hypothetical_descriptions)} hypothetical descriptions...\")\n",
    "query_embeddings = encoder.encode(\n",
    "    hypothetical_descriptions, batch_size=32, show_progress_bar=True,\n",
    "    convert_to_numpy=True, normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Search top 100\n",
    "k = 100\n",
    "print(f\"Searching top-{k}...\")\n",
    "scores, indices = index.search(query_embeddings.astype('float32'), k)\n",
    "\n",
    "results = {qid: [table_ids[idx] for idx in indices[i]] for i, qid in enumerate(query_ids)}\n",
    "print(f\"✓ Retrieved {len(results)} query results\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE RESULTS (HyDE)\")\n",
    "print(\"=\" * 80)\n",
    "for qid in list(queries.keys())[:3]:\n",
    "    print(f\"\\nQuery {qid}: '{queries[qid]}'\")\n",
    "    print(f\"Hypothetical: {hypothetical_descriptions[query_ids.index(qid)][:80]}...\")\n",
    "    for rank, tid in enumerate(results[qid][:3], 1):\n",
    "        rel = qrels.get(qid, {}).get(tid, 0)\n",
    "        score_val = scores[query_ids.index(qid)][rank-1]\n",
    "        page = tables_df[tables_df['table_id'] == tid].iloc[0]['page_title']\n",
    "        print(f\"  {rank}. {tid} (score: {score_val:.3f}, rel: {rel}) - {page}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    retrieved_at_k = set(retrieved[:k])\n",
    "    return len(retrieved_at_k & relevant) / len(relevant)\n",
    "\n",
    "def ndcg_at_k(retrieved, relevance, k):\n",
    "    if len(relevance) == 0:\n",
    "        return 0.0\n",
    "    dcg = sum(relevance.get(retrieved[i], 0) / np.log2(i + 2) for i in range(min(k, len(retrieved))))\n",
    "    ideal_rels = sorted(relevance.values(), reverse=True)[:k]\n",
    "    idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "k_values = [1, 5, 10, 20]\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for query_id, retrieved in results.items():\n",
    "    if query_id not in qrels:\n",
    "        continue\n",
    "    relevance = qrels[query_id]\n",
    "    relevant = set(tid for tid, rel in relevance.items() if rel > 0)\n",
    "    \n",
    "    for k in k_values:\n",
    "        metrics[f'Recall@{k}'].append(recall_at_k(retrieved, relevant, k))\n",
    "        metrics[f'nDCG@{k}'].append(ndcg_at_k(retrieved, relevance, k))\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYDE EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRecall:\")\n",
    "for k in k_values:\n",
    "    print(f\"  Recall@{k:2d}: {np.mean(metrics[f'Recall@{k}']):.4f}\")\n",
    "print(\"\\nnDCG:\")\n",
    "for k in k_values:\n",
    "    print(f\"  nDCG@{k:2d}  : {np.mean(metrics[f'nDCG@{k}']):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect specific query results\n",
    "query_id = '1'\n",
    "\n",
    "print(f\"Query {query_id}: {queries[query_id]}\")\n",
    "print(f\"Hypothetical description: {hypothetical_descriptions[query_ids.index(query_id)]}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, table_id in enumerate(results[query_id][:5], 1):\n",
    "    row = tables_df[tables_df['table_id'] == table_id].iloc[0]\n",
    "    rel = qrels.get(query_id, {}).get(table_id, 0)\n",
    "    score_val = scores[query_ids.index(query_id)][i-1]\n",
    "    \n",
    "    print(f\"\\n{i}. {table_id} (score: {score_val:.4f}, relevance: {rel})\")\n",
    "    print(f\"   Page: {row['page_title']}\")\n",
    "    print(f\"   Section: {row['section_title']}\")\n",
    "    print(f\"   Caption: {row['table_caption']}\")\n",
    "    \n",
    "    try:\n",
    "        headers = json.loads(row['headers'])\n",
    "        print(f\"   Headers: {headers[:5]}{'...' if len(headers) > 5 else ''}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        sample = json.loads(row['sample_data'])\n",
    "        print(f\"   Sample ({len(sample)} rows): {sample[0][:3]}...\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f\"   LLM Description: {table_descriptions[table_ids.index(table_id)]}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
